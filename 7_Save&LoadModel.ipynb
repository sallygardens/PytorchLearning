{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbb65386",
   "metadata": {},
   "source": [
    "# SAVE AND LOAD THE MODEL\n",
    "\n",
    "在这一节，我们会学习如何保持model state，保存、加载、运行model。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a78ca570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.onnx as onnx\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816929fb",
   "metadata": {},
   "source": [
    "## 保存和加载Model Weights\n",
    "\n",
    "Pytorch的model在一个内部状态字典中存储学习到的parameter，叫做<kbd>state_dict</kbd>,使用<kbd>torch.save</kbd>方法可以保存这些parameter。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cb3d8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to C:\\Users\\Lenovo/.cache\\torch\\hub\\checkpoints\\vgg16-397923af.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a08c9310cb34d8da5951c237d0f1e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/528M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = models.vgg16(pretrained=True)\n",
    "torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b2fb87",
   "metadata": {},
   "source": [
    "加载model weights，你需要创建一个相同model的实例，然后使用<kbd>load_state_dict()</kbd>方法来加载这些parameter。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fdd81aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.vgg16() # we do not specify pretrained=True, i.e. do not load default weights\n",
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9139c9",
   "metadata": {},
   "source": [
    "NOTE\n",
    "\n",
    "be sure to call <kbd>model.eval()</kbd> method before inferencing to set the dropout and batch normalization layers to evaluation mode. Failing to do this will yield inconsistent inference results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1972c3",
   "metadata": {},
   "source": [
    "## 保存和加载model以及model的形状\n",
    "\n",
    "当我们加载model weight的时候，我们需要实例化model class，因为这个class定义了network的结构，我们可能想把这个network的结构和model一起保存，我们可以将<kbd>model</kbd>(不是<kbd>model.state_dict()</kbd>)传入到save函数中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09d375be",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a66551c",
   "metadata": {},
   "source": [
    "使用如下方法加载model："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7138eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7885b96",
   "metadata": {},
   "source": [
    "NOTE\n",
    "\n",
    "This approach uses Python pickle module when serializing the model, thus it relies on the actual class definition to be available when loading the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1749b218",
   "metadata": {},
   "source": [
    "## model导出为ONNX\n",
    "\n",
    "Pytorch提供原生的ONNX导出支持，然而考虑到Pytorch execution graph的动态性，这个导出过程必须遍历这个execution graph来产生持久的ONNX model。出于这种考虑，需要将一个合适大小的test variable传入到导出的路线上(export routine)（在我们的例子中，我们将仿照着创建一个正确大小的zero tensor)："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3c6a49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Software\\Anaconda3\\envs\\py38\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.zeros((1,3,224,224))\n",
    "onnx.export(model, input_image, 'model.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0820e4d",
   "metadata": {},
   "source": [
    "你可以在ONNX model上做很多事情，包括在不同平台或者不同编程语言下运行，有关ONNX更多的细节可以看这个教程[ONNX tutorial](https://github.com/onnx/tutorials).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4673b14c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
