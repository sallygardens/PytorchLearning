{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa30649c",
   "metadata": {},
   "source": [
    "# Optimizing Model Parameters\n",
    "\n",
    "现在我们有了model和data，是时候通过依靠data更新model的参数来训练(train)、验证(validate)以及测试(test)我们的model了。训练一个model是一个反复(iterative)的过程;在每一次的迭代张红(一次epoch)，model会输出一个结果(guess about the output),计算它guess(loss)的error率，收集error的derivative，然后通过gradient descent去更新这些parameters让它们达到最优值。关于这个过程的更多细节，请过看这个视频[backpropagation from 3Blue1Brown](https://www.youtube.com/watch?v=tIeHLnjs5U8)\n",
    "\n",
    "## 预备代码\n",
    "下面的代码是在之前章节写过的代码。\n",
    "\n",
    "2.Datasets & DataLoaders\n",
    "\n",
    "4.Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7932e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Software\\Anaconda3\\envs\\py38\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b516d46e",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Hyperparameters是可调节的parameters，通过它让你可以控制model optimization的过程。不同的hyperparameter值可以影响model的训练和收敛点（convergence rate）（阅读关于更多的[hyperparameter tuning](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html)）\n",
    "\n",
    "我们在训练过程中定义了如下的hyperparameters：\n",
    "\n",
    "* **Number of Epoch**-在dataset上迭代的次数\n",
    "* **Batch Size**-在parameter更新前每次通过network的data sample数量\n",
    "* **Learning rate**-在每次的batch\\epoch更新模型parameter的幅度。值越小意味着更小的学习速率，值过大可能在训练过程中导致不可预料的事情。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "699c63ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=1e-3\n",
    "batch_size=64\n",
    "epoch=5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1594f20f",
   "metadata": {},
   "source": [
    "## Optimization Loop\n",
    "\n",
    "当我们设置好了hyperparameter后，我们可以在optimization循环中训练和更新我们的model，每次的循环中的一次迭代都叫做一次的**Epoch**。\n",
    "\n",
    "每次Epoch包含两个重要的部分：\n",
    "\n",
    "* **The Train Loop**-在training dataset上迭代，尽可能的收敛到parameter的最优点。\n",
    "* **The Validation/Test Loop**-在test dataset上迭代来检验model的表现是否有得到提升。\n",
    "\n",
    "我们简要的了解一下在training loop中的一些概念。\n",
    "\n",
    "### Loss函数\n",
    "我们没有训练过的network面对training data时候往往不能给出正确的答案。**Loss函数**就是度量通过network获得的结果和target value不同的程度，在训练的过程中，我们需要让loss函数的值能达到最低点。计算loss的方式一般是我们通过给定的input data值做出一个预测值，用它来比较data的label值。\n",
    "\n",
    "常见的loss function包括回归任务的[nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss)(Mean Square Error)和分类任务的[nn.NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss)(Negative Log Likelihood).[nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)将<kbd>nn.LogSoftmax</kbd>和<kbd>nn.NLLLoss</kbd>连接到了一起。\n",
    "\n",
    "我们将output logits输入到<kbd>nn.CrossEntropyLoss</kbd>，通过这样将logits标准化然后计算prediction error。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86f3c59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Loss function\n",
    "loss_fn=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275bfa24",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "Optimization是调整model parameters在每一次的training中降低model error的过程。Optimization算法定义了这个过程的执行方式（在这个例子中我们使用Stochastic Gradient Descent）。所有的optimization逻辑都封装在<kbd>optimizer object</kbd>中，这个例子中我们使用SGD optimizer；此外，在Pytorch中还有很多不同的Optimizer可以使用，例如ADAM和RMSProp，这些Optimizer在不同种类的model和data中可能会表现的更好。\n",
    "\n",
    "我们通过传入需要训练的model's parameter以及learning rate hyperparameter来初始化optimizer。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58bea422",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.SGD(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdfb318",
   "metadata": {},
   "source": [
    "在training loop内部optimization过程一共分三步：\n",
    "* 调用<kbd>optimizer.zero_grad()</kbd>重置model parameter的gradient。Gradient默认累加的；防止加两次，我们要在每次迭代的时候将它们清零。\n",
    "* 调用 <kbd>loss.backwards()</kbd>方法反向传播prediction loss。Pytorch会存储每个parameter的loss的gradient。\n",
    "* 得到gradient以后，我们调用 <kbd>optimizer.step()</kbd>函数通过收集backward pass过程中的gradient来调整parameter。\n",
    "\n",
    "## 全部代码\n",
    "\n",
    "我们定义<kbd>train_loop</kbd>来循环我们的optimization部分的代码，定义<kbd>test_loop</kbd>评估在test data上的model's performance。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24b50dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader,model,loss_fn,optimizer):\n",
    "    size=len(dataloader.dataset)\n",
    "    for batch,(X,y)in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred,y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch%100 == 0:\n",
    "            loss,current = loss.item(),batch*len(X)\n",
    "            print(f\"loss:{loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader,model,loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X,y in dataloader:\n",
    "            pred=model(X)\n",
    "            test_loss+=loss_fn(pred,y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9b8f2a",
   "metadata": {},
   "source": [
    "我们初始化loss函数和optimizer，然后将其加入到<kbd>train_loop</kbd>和<kbd>test_loop</kbd>。增加epoch次数并追踪model提升的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "825a205d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss:2.300437 [    0/60000]\n",
      "loss:2.296579 [ 6400/60000]\n",
      "loss:2.283700 [12800/60000]\n",
      "loss:2.284281 [19200/60000]\n",
      "loss:2.293092 [25600/60000]\n",
      "loss:2.278162 [32000/60000]\n",
      "loss:2.271635 [38400/60000]\n",
      "loss:2.258407 [44800/60000]\n",
      "loss:2.256768 [51200/60000]\n",
      "loss:2.267684 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 33.8%, Avg loss: 2.246487 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss:2.241510 [    0/60000]\n",
      "loss:2.252903 [ 6400/60000]\n",
      "loss:2.212900 [12800/60000]\n",
      "loss:2.221671 [19200/60000]\n",
      "loss:2.246316 [25600/60000]\n",
      "loss:2.224112 [32000/60000]\n",
      "loss:2.211269 [38400/60000]\n",
      "loss:2.189314 [44800/60000]\n",
      "loss:2.187185 [51200/60000]\n",
      "loss:2.217227 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.6%, Avg loss: 2.171180 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss:2.164196 [    0/60000]\n",
      "loss:2.187643 [ 6400/60000]\n",
      "loss:2.106689 [12800/60000]\n",
      "loss:2.122961 [19200/60000]\n",
      "loss:2.181246 [25600/60000]\n",
      "loss:2.150117 [32000/60000]\n",
      "loss:2.123044 [38400/60000]\n",
      "loss:2.091353 [44800/60000]\n",
      "loss:2.087848 [51200/60000]\n",
      "loss:2.146300 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 38.7%, Avg loss: 2.065942 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss:2.061570 [    0/60000]\n",
      "loss:2.100028 [ 6400/60000]\n",
      "loss:1.967387 [12800/60000]\n",
      "loss:1.992769 [19200/60000]\n",
      "loss:2.106472 [25600/60000]\n",
      "loss:2.064496 [32000/60000]\n",
      "loss:2.021308 [38400/60000]\n",
      "loss:1.986701 [44800/60000]\n",
      "loss:1.991558 [51200/60000]\n",
      "loss:2.078174 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 40.1%, Avg loss: 1.963642 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss:1.963543 [    0/60000]\n",
      "loss:2.015622 [ 6400/60000]\n",
      "loss:1.841800 [12800/60000]\n",
      "loss:1.880054 [19200/60000]\n",
      "loss:2.040149 [25600/60000]\n",
      "loss:1.993274 [32000/60000]\n",
      "loss:1.940838 [38400/60000]\n",
      "loss:1.909086 [44800/60000]\n",
      "loss:1.916781 [51200/60000]\n",
      "loss:2.027921 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 43.2%, Avg loss: 1.885069 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss:1.885678 [    0/60000]\n",
      "loss:1.947842 [ 6400/60000]\n",
      "loss:1.748060 [12800/60000]\n",
      "loss:1.796084 [19200/60000]\n",
      "loss:1.979340 [25600/60000]\n",
      "loss:1.934553 [32000/60000]\n",
      "loss:1.880705 [38400/60000]\n",
      "loss:1.851220 [44800/60000]\n",
      "loss:1.857017 [51200/60000]\n",
      "loss:1.985698 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 45.8%, Avg loss: 1.822072 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss:1.821329 [    0/60000]\n",
      "loss:1.892924 [ 6400/60000]\n",
      "loss:1.675043 [12800/60000]\n",
      "loss:1.724005 [19200/60000]\n",
      "loss:1.924271 [25600/60000]\n",
      "loss:1.883087 [32000/60000]\n",
      "loss:1.831600 [38400/60000]\n",
      "loss:1.804111 [44800/60000]\n",
      "loss:1.807417 [51200/60000]\n",
      "loss:1.947954 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 46.9%, Avg loss: 1.769184 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss:1.765525 [    0/60000]\n",
      "loss:1.843922 [ 6400/60000]\n",
      "loss:1.614016 [12800/60000]\n",
      "loss:1.660801 [19200/60000]\n",
      "loss:1.875209 [25600/60000]\n",
      "loss:1.838089 [32000/60000]\n",
      "loss:1.789887 [38400/60000]\n",
      "loss:1.766840 [44800/60000]\n",
      "loss:1.766492 [51200/60000]\n",
      "loss:1.914253 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 47.9%, Avg loss: 1.725298 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss:1.718280 [    0/60000]\n",
      "loss:1.802363 [ 6400/60000]\n",
      "loss:1.563616 [12800/60000]\n",
      "loss:1.609884 [19200/60000]\n",
      "loss:1.834926 [25600/60000]\n",
      "loss:1.800444 [32000/60000]\n",
      "loss:1.757086 [38400/60000]\n",
      "loss:1.737923 [44800/60000]\n",
      "loss:1.732565 [51200/60000]\n",
      "loss:1.886925 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 48.6%, Avg loss: 1.690405 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss:1.679651 [    0/60000]\n",
      "loss:1.769398 [ 6400/60000]\n",
      "loss:1.523934 [12800/60000]\n",
      "loss:1.569897 [19200/60000]\n",
      "loss:1.803294 [25600/60000]\n",
      "loss:1.771080 [32000/60000]\n",
      "loss:1.730791 [38400/60000]\n",
      "loss:1.715881 [44800/60000]\n",
      "loss:1.705337 [51200/60000]\n",
      "loss:1.864640 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 49.1%, Avg loss: 1.662953 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb34d06d",
   "metadata": {},
   "source": [
    "## 拓展阅读\n",
    "* [Loss Functions](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
    "* [torch.optim](https://pytorch.org/docs/stable/optim.html)\n",
    "* [Warmstart Training a Model](https://pytorch.org/tutorials/recipes/recipes/warmstarting_model_using_parameters_from_a_different_model.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb6a3d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38]",
   "language": "python",
   "name": "conda-env-py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
